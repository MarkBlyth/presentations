#+OPTIONS: H:2 toc:nil
#+LATEX_CLASS: beamer
#+COLUMNS: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+BEAMER_THEME: UoB
#+AUTHOR: Mark Blyth
#+TITLE: Spring project summary
#+DATE: [2020-05-25 Mon]

* Previous
** Last meeting
Discussion about single-cell and multi-cell approaches
\vfill
Single-cell: 
#+ATTR_LATEX: :overlay [<+->]
    * Strong literature precedent for what to expect
    * Lots of accepted models to test /in-silico/
    * Easy-to-spot bifurcations
      * Hopf, fold both easily detectable with CBC
    * Reuse Bath single-cell microfluidics device


** Last meeting
Discussion about single-cell and multi-cell approaches
\vfill
Multi-cell: 
#+ATTR_LATEX: :overlay [<+->]
    * Assume there's an arbitrarily large number of cells
      * Neural continuum fields (limited descriptive ability)
      * Spatially extended cubic Lienard system
    * Search experimentally and numerically for PDE bifurcations
      * Not an area I know much about (yet...)
    * Can build on work by Krasi's Munich collaborators
    * Or could reuse Bath microfluidic device
      * Would require minor alterations to increase spatial resolution


** Single- vs multi-cell
Deciding factors:

\vfill
   
    * No lab access for the forseeable future
      * Work can be guided less by experiments
    * Single-cell easier than multi-cell
      * I know enough about single-cell CBC to start working on it

\vfill

Conclusion: work on single-cell case


* Current
** Current goals
   * Single-cell /in-silico/ CBC
   * Tutorial-review paper for numerical continuation
   
** Challenges of /in-silico/ CBC
Data aren't ideal to work with:
\vfill
   * Real signals are noise-corrupted
     * Difficult to filter off, since spikes contain lots of high-frequency components
     * Hard to run continuation on stochastic and noisy signals
\vfill
   * Neurons are fast-spiking
     * Fourier discretisation won't work
     * Discretisations need to be very high-dimensional, making Jacobian very slow to find
       
\vfill
       
** Issue 1: noise corruption
Instead of running continuation on noisy signal measurements, let's run it on a surrogate data source
#+ATTR_LATEX: :overlay [<+->]
    * Assume no knowledge of the system
      * CBC is a model-free method
    * Come up with some surrogate model to replace the observed data
      * Must filter out the noise
      * Must handle experimental issues (missing measurements, uneven spacing, small amounts of data)
    * Surrogate model can then be analysed as desired
      * Smooth, clean datasource (no noise, no missing points, full interpolation)
      * Accurate derivatives, of arbitrary order
      * Allows for accurate delay embeddings, collocation discretisation -- conventional continuation can be used on it
	
\vfill

** Candidate surrogate models
#+ATTR_LATEX: :overlay [<+->]
    * Truncated Fourier series
      * Currently used in CBC
      * Bad for noisy or spiking data
    * Wavelet decomposition
      * `Enveloped' oscillations
    * Splines
      * Piecewise-polynomial model
    * Gaussian process regression
      * Statistically optimal, when noise is Gaussian
	
These require no preexisting knowledge /[loosely speaking]/, and work well with sparse data
     
** Gaussian process regression
#+ATTR_LATEX: :width .9\textwidth
[[./GPR_demo.pdf]]

GPR can recover the underlying signal from noise-corrupted observations

** Surrogate models

#+ATTR_LATEX: :width .9\textwidth
[[./HHraw.pdf]]

GP surrogate models don't always work well!

** Surrogate models

#+ATTR_LATEX: :width .9\textwidth
[[./HH_FKL_test.pdf]]
GP surrogate models don't always work well!

** Machine learning for dynamical systems
#+ATTR_LATEX: :overlay [<+->]
   * Current approach: Gaussian process regression; predict new points as an intelligently weighted sum of example points
   * Bayesian kernel method
     * Kernel specifies a distribution over basis functions
     * Good kernel choice = good data fit
   * Most kernels are stationary
     * Assumes statistical properties are time-invariant /[they're not]/
     * Can't handle the spiking behaviours of neurons
     
\vfill

Current goal: find a good approach to fitting a surrogate model


* Next
  
** Next questions
   * Surrogate models on real data
   * Predictor-corrector design
   * Stochastic models

** Continuation issues
    * Discretisation is required to make predictor-corrector methods work
      * Can't run continuation on a function; must discretise it into a vector
    * Discretisation has issues when used on fast-spiking data
      * Requires lots of datapoints
      * Slow to find a Jacobian for Newton-iterations
      * High noise-sensitivity
    * Surrogate models and discretisation-free predictor-correctors might help overcome these

** Alternative continuation approach
Predictor-corrector design:
\vfill
    * We could try discretisation-free predictor steps, using a surrogate model
      * Let \(f_i(t)\) be the surrogate model for system behaviours at parameter \(\lambda_i\)
      * Given periodic orbits \(f_{i-1},~f_i\), predict \(f_{i+1} = f_i + h \big[f_i - f_{i-1}\big]\)
\vfill
    * Corrector step would be harder
	
** COMMENT An idea for discretisation-free correction
Main goal of CBC: find \(x^*(t)\) such that \(\forall t, u(x,x^*)=0\).

\vfill
Alternative formulation:
#+ATTR_LATEX: :overlay [<+->]
     * Let \(S[x^*] = \int_0^T u^2(x,x^*) \mathrm{d}t\) measure control invasiveness
     * \(S: \mathcal{H} \to \mathbb{R}\) is a functional on control actions \(x^*\)
     * CBC becomes a calculus of variations problem; find \(x^*(t)\) that minimises \(S\)
     * \(S=0\) if and only if \(x^*(t)\) is an invariant set of the open-loop system

\vfill
** COMMENT Calculus of variations
Alternative formulation: find \(x^*(t)\) that minimises \(S[x^*]\)

\vfill

     * Calculus of variations provides a framework for finding minimising functions
     * Might be possible to define an iteration scheme on functions, rather than discretisations

\vfill
Calculus of variations
     * Well-studied in control theory; lots of precedent to build on
     * Shifts the noninvasiveness requirement away from the continuation scheme, and onto the controller
       
** COMMENT Variational noninvasiveness
Ideally, corrector would find some iteration sequence \(f_1,~f_2,~\dots\), such that \(S[f_i] > S[f_{i-1}]\)
    * Then we've found a function-space iteration scheme to reach noninvasive control
    * Works on functions at every step, so we avoid the issues of discretisation
\vfill
Might be a dead-end.

** COMMENT Variational noninvasiveness
Overall idea:
    * Set up CBC as a calculus of variations problem
    * Reach noninvasiveness by minimising functional \(S\)
    * Find a numerical method to do this though iterations on control target \(x^*(t)\)
    * Use the variational equations to reformulate Newton iterations onto functions, rather than vectors
      * Main question: is this even possible?

** Stochastic models
Another challenge: real neurons are stochastic
\vfill
    * Stochasticity introduces new challenges
      * Coherence and stochastic resonance
      * Random attractors
      * Stochastic calculus
      * Not an area I know much about /[yet...]/
    * Current work: CBC on noise-corrupted simulations
    * Next work: CBC on truly stochastic models

\vfill

Big question: how different would truly stochastic models be?


** Goals
Actions:
    * Find a surrogate modelling method for neural data
    * Attempt a discretisation-free corrector?
    * Run CBC on deterministic models, then stochastic

\vfill

Results:
   * Write up surrogate modelling into a conference abstract /[July]/
     * Maybe a conference paper /[September]/
   * Use surrogate modelling for an /in-silico/ CBC paper /[next year?]/

     
* Normal supervision slides
** BONUS: Week's work
#+ATTR_LATEX: :overlay [<+->]
   * Functional kernel learning now works
     * Stationary kernel method
     * Performs well on Fitzhugh-Nagumo 
     * Performs badly on Hodgkin-Huxley
   * New model validation method
     * Run a high-accuracy solver, for lots of datapoints
     * Downsample
     * Train models on half the datapoints, test them on the other half
     * Could optionally do an error integral, since we have a continuous model
   * Looked into noise-training
     * Couldn't find anything
   * Non-stationary kernel is not working
   * Support vector regression

** BONUS: SVR
#+ATTR_LATEX: :overlay [<+->]
 * SVR is the regression-equivalent of a support vector machine
   * Another popular kernel method
 * It works moderately well on neuron data
   * Fast!
   * Fair performance on non-stationary data
   * Doesn't always average out noise well
 * Another idea: ensemble models
   * Fit a few different models (GPR, splines, SVR, etc)
   * Use something analogous to sensory fusion, to combine model predictions
   * Gradient boosting: combine several weak learners to make a single strong learner
     
Only worth trying after I've tested all the other regression methods

** BONUS: SVR

#+ATTR_LATEX: :width \textwidth
[[./SVR.pdf]]
